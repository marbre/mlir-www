<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>'nvvm' Dialect - MLIR</title><meta name=description content="Multi-Level IR Compiler Framework"><meta name=generator content="Hugo 0.80.0"><link href=https://mlir.llvm.org/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://mlir.llvm.org/docs/Dialects/NVVMDialect/><link rel=stylesheet href=https://mlir.llvm.org/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script><link rel=stylesheet href=https://mlir.llvm.org/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script><script src=https://mlir.llvm.org/js/bundle.js></script><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
    }
  });
</script><link rel=icon href=/favicon.svg type=image/svg+xml sizes=any><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://mlir.llvm.org//mlir-logo.png width=40px align=absmiddle>
MLIR</div></h1><p class=description>Multi-Level IR Compiler Framework</p></header><div class=global-menu><nav><ul><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/mlir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/getting_started/Debugging/>Debugging</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/llvm/llvm-project/tree/main/mlir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=/doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/llvm/llvm-project/tree/main/mlir>GitHub</a></li></ul></li><li><a href="https://bugs.llvm.org/buglist.cgi?bug_status=__open__&list_id=177877&order=changeddate%20DESC%2Cpriority%2Cbug_severity&product=MLIR&query_format=specific">Bugs</a></li><li><a href=/LogoAssets/>Logo Assets</a></li></ul></nav></div><div class=content-container><main><h1>'nvvm' Dialect</h1><p><nav id=TableOfContents><ul><li><a href=#operation-definition>Operation definition</a><ul><li><a href=#nvvmbarrier0-mlirnvvmbarrier0op><code>nvvm.barrier0</code> (::mlir::NVVM::Barrier0Op)</a></li><li><a href=#nvvmreadptxsregntidx-mlirnvvmblockdimxop><code>nvvm.read.ptx.sreg.ntid.x</code> (::mlir::NVVM::BlockDimXOp)</a></li><li><a href=#nvvmreadptxsregntidy-mlirnvvmblockdimyop><code>nvvm.read.ptx.sreg.ntid.y</code> (::mlir::NVVM::BlockDimYOp)</a></li><li><a href=#nvvmreadptxsregntidz-mlirnvvmblockdimzop><code>nvvm.read.ptx.sreg.ntid.z</code> (::mlir::NVVM::BlockDimZOp)</a></li><li><a href=#nvvmreadptxsregctaidx-mlirnvvmblockidxop><code>nvvm.read.ptx.sreg.ctaid.x</code> (::mlir::NVVM::BlockIdXOp)</a></li><li><a href=#nvvmreadptxsregctaidy-mlirnvvmblockidyop><code>nvvm.read.ptx.sreg.ctaid.y</code> (::mlir::NVVM::BlockIdYOp)</a></li><li><a href=#nvvmreadptxsregctaidz-mlirnvvmblockidzop><code>nvvm.read.ptx.sreg.ctaid.z</code> (::mlir::NVVM::BlockIdZOp)</a></li><li><a href=#nvvmreadptxsregnctaidx-mlirnvvmgriddimxop><code>nvvm.read.ptx.sreg.nctaid.x</code> (::mlir::NVVM::GridDimXOp)</a></li><li><a href=#nvvmreadptxsregnctaidy-mlirnvvmgriddimyop><code>nvvm.read.ptx.sreg.nctaid.y</code> (::mlir::NVVM::GridDimYOp)</a></li><li><a href=#nvvmreadptxsregnctaidz-mlirnvvmgriddimzop><code>nvvm.read.ptx.sreg.nctaid.z</code> (::mlir::NVVM::GridDimZOp)</a></li><li><a href=#nvvmreadptxsreglaneid-mlirnvvmlaneidop><code>nvvm.read.ptx.sreg.laneid</code> (::mlir::NVVM::LaneIdOp)</a></li><li><a href=#nvvmmmasync-mlirnvvmmmaop><code>nvvm.mma.sync</code> (::mlir::NVVM::MmaOp)</a></li><li><a href=#nvvmshflsyncbfly-mlirnvvmshflbflyop><code>nvvm.shfl.sync.bfly</code> (::mlir::NVVM::ShflBflyOp)</a></li><li><a href=#nvvmreadptxsregtidx-mlirnvvmthreadidxop><code>nvvm.read.ptx.sreg.tid.x</code> (::mlir::NVVM::ThreadIdXOp)</a></li><li><a href=#nvvmreadptxsregtidy-mlirnvvmthreadidyop><code>nvvm.read.ptx.sreg.tid.y</code> (::mlir::NVVM::ThreadIdYOp)</a></li><li><a href=#nvvmreadptxsregtidz-mlirnvvmthreadidzop><code>nvvm.read.ptx.sreg.tid.z</code> (::mlir::NVVM::ThreadIdZOp)</a></li><li><a href=#nvvmvoteballotsync-mlirnvvmvoteballotop><code>nvvm.vote.ballot.sync</code> (::mlir::NVVM::VoteBallotOp)</a></li><li><a href=#nvvmwmmam16n16k16loadaf16rowstride-mlirnvvmwmmaloadam16n16k16op><code>nvvm.wmma.m16n16k16.load.a.f16.row.stride</code> (::mlir::NVVM::WMMALoadAM16N16K16Op)</a></li><li><a href=#nvvmwmmam16n16k16loadbf16rowstride-mlirnvvmwmmaloadbm16n16k16op><code>nvvm.wmma.m16n16k16.load.b.f16.row.stride</code> (::mlir::NVVM::WMMALoadBM16N16K16Op)</a></li><li><a href=#nvvmwmmam16n16k16loadcf16rowstride-mlirnvvmwmmaloadcf16m16n16k16op><code>nvvm.wmma.m16n16k16.load.c.f16.row.stride</code> (::mlir::NVVM::WMMALoadCF16M16N16K16Op)</a></li><li><a href=#nvvmwmmam16n16k16loadcf32rowstride-mlirnvvmwmmaloadcf32m16n16k16op><code>nvvm.wmma.m16n16k16.load.c.f32.row.stride</code> (::mlir::NVVM::WMMALoadCF32M16N16K16Op)</a></li><li><a href=#nvvmwmmam16n16k16mmarowrowf16f16-mlirnvvmwmmammaf16f16m16n16k16op><code>nvvm.wmma.m16n16k16.mma.row.row.f16.f16</code> (::mlir::NVVM::WMMAMmaF16F16M16N16K16Op)</a></li><li><a href=#nvvmwmmam16n16k16mmarowrowf32f32-mlirnvvmwmmammaf32f32m16n16k16op><code>nvvm.wmma.m16n16k16.mma.row.row.f32.f32</code> (::mlir::NVVM::WMMAMmaF32F32M16N16K16Op)</a></li><li><a href=#nvvmwmmam16n16k16storedf16rowstride-mlirnvvmwmmastoref16m16n16k16op><code>nvvm.wmma.m16n16k16.store.d.f16.row.stride</code> (::mlir::NVVM::WMMAStoreF16M16N16K16Op)</a></li><li><a href=#nvvmwmmam16n16k16storedf32rowstride-mlirnvvmwmmastoref32m16n16k16op><code>nvvm.wmma.m16n16k16.store.d.f32.row.stride</code> (::mlir::NVVM::WMMAStoreF32M16N16K16Op)</a></li><li><a href=#nvvmreadptxsregwarpsize-mlirnvvmwarpsizeop><code>nvvm.read.ptx.sreg.warpsize</code> (::mlir::NVVM::WarpSizeOp)</a></li></ul></li></ul></nav><h2 id=operation-definition>Operation definition&nbsp;<a class=headline-hash href=#operation-definition>¶</a></h2><h3 id=nvvmbarrier0-mlirnvvmbarrier0op><code>nvvm.barrier0</code> (::mlir::NVVM::Barrier0Op)&nbsp;<a class=headline-hash href=#nvvmbarrier0-mlirnvvmbarrier0op>¶</a></h3><p>Syntax:</p><pre><code>operation ::= `nvvm.barrier0` attr-dict
</code></pre><h3 id=nvvmreadptxsregntidx-mlirnvvmblockdimxop><code>nvvm.read.ptx.sreg.ntid.x</code> (::mlir::NVVM::BlockDimXOp)&nbsp;<a class=headline-hash href=#nvvmreadptxsregntidx-mlirnvvmblockdimxop>¶</a></h3><p>Syntax:</p><pre><code>operation ::= `nvvm.read.ptx.sreg.ntid.x` attr-dict `:` type($res)
</code></pre><h4 id=results>Results:&nbsp;<a class=headline-hash href=#results>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h3 id=nvvmreadptxsregntidy-mlirnvvmblockdimyop><code>nvvm.read.ptx.sreg.ntid.y</code> (::mlir::NVVM::BlockDimYOp)&nbsp;<a class=headline-hash href=#nvvmreadptxsregntidy-mlirnvvmblockdimyop>¶</a></h3><p>Syntax:</p><pre><code>operation ::= `nvvm.read.ptx.sreg.ntid.y` attr-dict `:` type($res)
</code></pre><h4 id=results-1>Results:&nbsp;<a class=headline-hash href=#results-1>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h3 id=nvvmreadptxsregntidz-mlirnvvmblockdimzop><code>nvvm.read.ptx.sreg.ntid.z</code> (::mlir::NVVM::BlockDimZOp)&nbsp;<a class=headline-hash href=#nvvmreadptxsregntidz-mlirnvvmblockdimzop>¶</a></h3><p>Syntax:</p><pre><code>operation ::= `nvvm.read.ptx.sreg.ntid.z` attr-dict `:` type($res)
</code></pre><h4 id=results-2>Results:&nbsp;<a class=headline-hash href=#results-2>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h3 id=nvvmreadptxsregctaidx-mlirnvvmblockidxop><code>nvvm.read.ptx.sreg.ctaid.x</code> (::mlir::NVVM::BlockIdXOp)&nbsp;<a class=headline-hash href=#nvvmreadptxsregctaidx-mlirnvvmblockidxop>¶</a></h3><p>Syntax:</p><pre><code>operation ::= `nvvm.read.ptx.sreg.ctaid.x` attr-dict `:` type($res)
</code></pre><h4 id=results-3>Results:&nbsp;<a class=headline-hash href=#results-3>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h3 id=nvvmreadptxsregctaidy-mlirnvvmblockidyop><code>nvvm.read.ptx.sreg.ctaid.y</code> (::mlir::NVVM::BlockIdYOp)&nbsp;<a class=headline-hash href=#nvvmreadptxsregctaidy-mlirnvvmblockidyop>¶</a></h3><p>Syntax:</p><pre><code>operation ::= `nvvm.read.ptx.sreg.ctaid.y` attr-dict `:` type($res)
</code></pre><h4 id=results-4>Results:&nbsp;<a class=headline-hash href=#results-4>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h3 id=nvvmreadptxsregctaidz-mlirnvvmblockidzop><code>nvvm.read.ptx.sreg.ctaid.z</code> (::mlir::NVVM::BlockIdZOp)&nbsp;<a class=headline-hash href=#nvvmreadptxsregctaidz-mlirnvvmblockidzop>¶</a></h3><p>Syntax:</p><pre><code>operation ::= `nvvm.read.ptx.sreg.ctaid.z` attr-dict `:` type($res)
</code></pre><h4 id=results-5>Results:&nbsp;<a class=headline-hash href=#results-5>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h3 id=nvvmreadptxsregnctaidx-mlirnvvmgriddimxop><code>nvvm.read.ptx.sreg.nctaid.x</code> (::mlir::NVVM::GridDimXOp)&nbsp;<a class=headline-hash href=#nvvmreadptxsregnctaidx-mlirnvvmgriddimxop>¶</a></h3><p>Syntax:</p><pre><code>operation ::= `nvvm.read.ptx.sreg.nctaid.x` attr-dict `:` type($res)
</code></pre><h4 id=results-6>Results:&nbsp;<a class=headline-hash href=#results-6>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h3 id=nvvmreadptxsregnctaidy-mlirnvvmgriddimyop><code>nvvm.read.ptx.sreg.nctaid.y</code> (::mlir::NVVM::GridDimYOp)&nbsp;<a class=headline-hash href=#nvvmreadptxsregnctaidy-mlirnvvmgriddimyop>¶</a></h3><p>Syntax:</p><pre><code>operation ::= `nvvm.read.ptx.sreg.nctaid.y` attr-dict `:` type($res)
</code></pre><h4 id=results-7>Results:&nbsp;<a class=headline-hash href=#results-7>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h3 id=nvvmreadptxsregnctaidz-mlirnvvmgriddimzop><code>nvvm.read.ptx.sreg.nctaid.z</code> (::mlir::NVVM::GridDimZOp)&nbsp;<a class=headline-hash href=#nvvmreadptxsregnctaidz-mlirnvvmgriddimzop>¶</a></h3><p>Syntax:</p><pre><code>operation ::= `nvvm.read.ptx.sreg.nctaid.z` attr-dict `:` type($res)
</code></pre><h4 id=results-8>Results:&nbsp;<a class=headline-hash href=#results-8>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h3 id=nvvmreadptxsreglaneid-mlirnvvmlaneidop><code>nvvm.read.ptx.sreg.laneid</code> (::mlir::NVVM::LaneIdOp)&nbsp;<a class=headline-hash href=#nvvmreadptxsreglaneid-mlirnvvmlaneidop>¶</a></h3><p>Syntax:</p><pre><code>operation ::= `nvvm.read.ptx.sreg.laneid` attr-dict `:` type($res)
</code></pre><h4 id=results-9>Results:&nbsp;<a class=headline-hash href=#results-9>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h3 id=nvvmmmasync-mlirnvvmmmaop><code>nvvm.mma.sync</code> (::mlir::NVVM::MmaOp)&nbsp;<a class=headline-hash href=#nvvmmmasync-mlirnvvmmmaop>¶</a></h3><p>Syntax:</p><pre><code>operation ::= `nvvm.mma.sync` $args attr-dict `:` functional-type($args, $res)
</code></pre><h4 id=operands>Operands:&nbsp;<a class=headline-hash href=#operands>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>args</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h4 id=results-10>Results:&nbsp;<a class=headline-hash href=#results-10>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h3 id=nvvmshflsyncbfly-mlirnvvmshflbflyop><code>nvvm.shfl.sync.bfly</code> (::mlir::NVVM::ShflBflyOp)&nbsp;<a class=headline-hash href=#nvvmshflsyncbfly-mlirnvvmshflbflyop>¶</a></h3><h4 id=attributes>Attributes:&nbsp;<a class=headline-hash href=#attributes>¶</a></h4><table><thead><tr><th style=text-align:center>Attribute</th><th style=text-align:center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>return_value_and_is_valid</code></td><td style=text-align:center>::mlir::UnitAttr</td><td>unit attribute</td></tr></tbody></table><h4 id=operands-1>Operands:&nbsp;<a class=headline-hash href=#operands-1>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>dst</code></td><td>LLVM dialect-compatible type</td></tr><tr><td style=text-align:center><code>val</code></td><td>LLVM dialect-compatible type</td></tr><tr><td style=text-align:center><code>offset</code></td><td>LLVM dialect-compatible type</td></tr><tr><td style=text-align:center><code>mask_and_clamp</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h4 id=results-11>Results:&nbsp;<a class=headline-hash href=#results-11>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h3 id=nvvmreadptxsregtidx-mlirnvvmthreadidxop><code>nvvm.read.ptx.sreg.tid.x</code> (::mlir::NVVM::ThreadIdXOp)&nbsp;<a class=headline-hash href=#nvvmreadptxsregtidx-mlirnvvmthreadidxop>¶</a></h3><p>Syntax:</p><pre><code>operation ::= `nvvm.read.ptx.sreg.tid.x` attr-dict `:` type($res)
</code></pre><h4 id=results-12>Results:&nbsp;<a class=headline-hash href=#results-12>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h3 id=nvvmreadptxsregtidy-mlirnvvmthreadidyop><code>nvvm.read.ptx.sreg.tid.y</code> (::mlir::NVVM::ThreadIdYOp)&nbsp;<a class=headline-hash href=#nvvmreadptxsregtidy-mlirnvvmthreadidyop>¶</a></h3><p>Syntax:</p><pre><code>operation ::= `nvvm.read.ptx.sreg.tid.y` attr-dict `:` type($res)
</code></pre><h4 id=results-13>Results:&nbsp;<a class=headline-hash href=#results-13>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h3 id=nvvmreadptxsregtidz-mlirnvvmthreadidzop><code>nvvm.read.ptx.sreg.tid.z</code> (::mlir::NVVM::ThreadIdZOp)&nbsp;<a class=headline-hash href=#nvvmreadptxsregtidz-mlirnvvmthreadidzop>¶</a></h3><p>Syntax:</p><pre><code>operation ::= `nvvm.read.ptx.sreg.tid.z` attr-dict `:` type($res)
</code></pre><h4 id=results-14>Results:&nbsp;<a class=headline-hash href=#results-14>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h3 id=nvvmvoteballotsync-mlirnvvmvoteballotop><code>nvvm.vote.ballot.sync</code> (::mlir::NVVM::VoteBallotOp)&nbsp;<a class=headline-hash href=#nvvmvoteballotsync-mlirnvvmvoteballotop>¶</a></h3><h4 id=operands-2>Operands:&nbsp;<a class=headline-hash href=#operands-2>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>mask</code></td><td>LLVM dialect-compatible type</td></tr><tr><td style=text-align:center><code>pred</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h4 id=results-15>Results:&nbsp;<a class=headline-hash href=#results-15>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h3 id=nvvmwmmam16n16k16loadaf16rowstride-mlirnvvmwmmaloadam16n16k16op><code>nvvm.wmma.m16n16k16.load.a.f16.row.stride</code> (::mlir::NVVM::WMMALoadAM16N16K16Op)&nbsp;<a class=headline-hash href=#nvvmwmmam16n16k16loadaf16rowstride-mlirnvvmwmmaloadam16n16k16op>¶</a></h3><p>Warp synchronous matrix load</p><p>Syntax:</p><pre><code>operation ::= `nvvm.wmma.m16n16k16.load.a.f16.row.stride` $args attr-dict `:` functional-type($args, $res)
</code></pre><p>&ldquo;The <code>nvvm.wmma.m*n*k*.load.[a, b, c]</code> operation&rdquo;
&ldquo;loads a matrix collectively using all the threads in a warp.&rdquo;</p><pre><code>&quot;The operation takes two arguments, the address from where the matrix&quot;
&quot;elements are to be loaded from and a stride. The stride argument&quot;
&quot;represents the leading dimension of the source matrix. The address and&quot;
&quot;the stride are required to be the same across all threads in the warp.&quot;
&quot;Each thread in a warp holds a certain number of elements. The Op returns&quot;
&quot;a LLVMStruct which holds the elements of the matrix held by this thread.&quot;

&quot;This op is meant to be used along with `nvvm.wmma.m*n*k*.store` and&quot;
&quot;`nvvm.wmma.m*n*k*.mma`.&quot;
Example:

```mlir
%2 = nvvm.wmma.m16n16k16.load.a %0, %1 : !llvm.ptr&lt;i32, 3&gt;, !llvm.i32 -&gt;
!llvm.struct&lt;(vec&lt;2 x half&gt;, vec&lt;2 x half&gt;, vec&lt;2 x half&gt;, vec&lt;2 x half&gt;,
vec&lt;2 x half&gt;, vec&lt;2 x half&gt;, vec&lt;2 x half&gt;, vec&lt;2 x half&gt;)&gt;
```
</code></pre><h4 id=operands-3>Operands:&nbsp;<a class=headline-hash href=#operands-3>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>args</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h4 id=results-16>Results:&nbsp;<a class=headline-hash href=#results-16>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM structure type</td></tr></tbody></table><h3 id=nvvmwmmam16n16k16loadbf16rowstride-mlirnvvmwmmaloadbm16n16k16op><code>nvvm.wmma.m16n16k16.load.b.f16.row.stride</code> (::mlir::NVVM::WMMALoadBM16N16K16Op)&nbsp;<a class=headline-hash href=#nvvmwmmam16n16k16loadbf16rowstride-mlirnvvmwmmaloadbm16n16k16op>¶</a></h3><p>Warp synchronous matrix load</p><p>Syntax:</p><pre><code>operation ::= `nvvm.wmma.m16n16k16.load.b.f16.row.stride` $args attr-dict `:` functional-type($args, $res)
</code></pre><p>&ldquo;The <code>nvvm.wmma.m*n*k*.load.[a, b, c]</code> operation&rdquo;
&ldquo;loads a matrix collectively using all the threads in a warp.&rdquo;</p><pre><code>&quot;The operation takes two arguments, the address from where the matrix&quot;
&quot;elements are to be loaded from and a stride. The stride argument&quot;
&quot;represents the leading dimension of the source matrix. The address and&quot;
&quot;the stride are required to be the same across all threads in the warp.&quot;
&quot;Each thread in a warp holds a certain number of elements. The Op returns&quot;
&quot;a LLVMStruct which holds the elements of the matrix held by this thread.&quot;

&quot;This op is meant to be used along with `nvvm.wmma.m*n*k*.store` and&quot;
&quot;`nvvm.wmma.m*n*k*.mma`.&quot;
Example:

```mlir
%2 = nvvm.wmma.m16n16k16.load.b %0, %1 : !llvm.ptr&lt;i32, 3&gt;, !llvm.i32 -&gt;
!llvm.struct&lt;(vec&lt;2 x half&gt;, vec&lt;2 x half&gt;, vec&lt;2 x half&gt;, vec&lt;2 x half&gt;,
vec&lt;2 x half&gt;, vec&lt;2 x half&gt;, vec&lt;2 x half&gt;, vec&lt;2 x half&gt;)&gt;
```
</code></pre><h4 id=operands-4>Operands:&nbsp;<a class=headline-hash href=#operands-4>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>args</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h4 id=results-17>Results:&nbsp;<a class=headline-hash href=#results-17>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM structure type</td></tr></tbody></table><h3 id=nvvmwmmam16n16k16loadcf16rowstride-mlirnvvmwmmaloadcf16m16n16k16op><code>nvvm.wmma.m16n16k16.load.c.f16.row.stride</code> (::mlir::NVVM::WMMALoadCF16M16N16K16Op)&nbsp;<a class=headline-hash href=#nvvmwmmam16n16k16loadcf16rowstride-mlirnvvmwmmaloadcf16m16n16k16op>¶</a></h3><p>Warp synchronous matrix load</p><p>Syntax:</p><pre><code>operation ::= `nvvm.wmma.m16n16k16.load.c.f16.row.stride` $args attr-dict `:` functional-type($args, $res)
</code></pre><p>&ldquo;The <code>nvvm.wmma.m*n*k*.load.[a, b, c]</code> operation&rdquo;
&ldquo;loads a matrix collectively using all the threads in a warp.&rdquo;</p><pre><code>&quot;The operation takes two arguments, the address from where the matrix&quot;
&quot;elements are to be loaded from and a stride. The stride argument&quot;
&quot;represents the leading dimension of the source matrix. The address and&quot;
&quot;the stride are required to be the same across all threads in the warp.&quot;
&quot;Each thread in a warp holds a certain number of elements. The Op returns&quot;
&quot;a LLVMStruct which holds the elements of the matrix held by this thread.&quot;

&quot;This op is meant to be used along with `nvvm.wmma.m*n*k*.store` and&quot;
&quot;`nvvm.wmma.m*n*k*.mma`.&quot;
Example:

```mlir
%2 = nvvm.wmma.m16n16k16.load.c.f16.row.stride %0, %1 : !llvm.ptr&lt;i32, 3&gt;, !llvm.i32 -&gt;
!llvm.struct&lt;(vec&lt;2 x half&gt;, vec&lt;2 x half&gt;, vec&lt;2 x half&gt;, vec&lt;2 x half&gt;)&gt;
```
</code></pre><h4 id=operands-5>Operands:&nbsp;<a class=headline-hash href=#operands-5>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>args</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h4 id=results-18>Results:&nbsp;<a class=headline-hash href=#results-18>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM structure type</td></tr></tbody></table><h3 id=nvvmwmmam16n16k16loadcf32rowstride-mlirnvvmwmmaloadcf32m16n16k16op><code>nvvm.wmma.m16n16k16.load.c.f32.row.stride</code> (::mlir::NVVM::WMMALoadCF32M16N16K16Op)&nbsp;<a class=headline-hash href=#nvvmwmmam16n16k16loadcf32rowstride-mlirnvvmwmmaloadcf32m16n16k16op>¶</a></h3><p>Warp synchronous matrix load</p><p>Syntax:</p><pre><code>operation ::= `nvvm.wmma.m16n16k16.load.c.f32.row.stride` $args attr-dict `:` functional-type($args, $res)
</code></pre><p>&ldquo;The <code>nvvm.wmma.m*n*k*.load.[a, b, c]</code> operation&rdquo;
&ldquo;loads a matrix collectively using all the threads in a warp.&rdquo;</p><pre><code>&quot;The operation takes two arguments, the address from where the matrix&quot;
&quot;elements are to be loaded from and a stride. The stride argument&quot;
&quot;represents the leading dimension of the source matrix. The address and&quot;
&quot;the stride are required to be the same across all threads in the warp.&quot;
&quot;Each thread in a warp holds a certain number of elements. The Op returns&quot;
&quot;a LLVMStruct which holds the elements of the matrix held by this thread.&quot;

&quot;This op is meant to be used along with `nvvm.wmma.m*n*k*.store` and&quot;
&quot;`nvvm.wmma.m*n*k*.mma`.&quot;
Example:

```mlir
%2 = nvvm.wmma.m16n16k16.load.c.f32.row.stride %0, %1 : !llvm.ptr&lt;i32, 3&gt;, !llvm.i32 -&gt;
!llvm.struct&lt;(f32, f32, f32, f32, f32, f32, f32, f32)&gt;
```
</code></pre><h4 id=operands-6>Operands:&nbsp;<a class=headline-hash href=#operands-6>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>args</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h4 id=results-19>Results:&nbsp;<a class=headline-hash href=#results-19>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM structure type</td></tr></tbody></table><h3 id=nvvmwmmam16n16k16mmarowrowf16f16-mlirnvvmwmmammaf16f16m16n16k16op><code>nvvm.wmma.m16n16k16.mma.row.row.f16.f16</code> (::mlir::NVVM::WMMAMmaF16F16M16N16K16Op)&nbsp;<a class=headline-hash href=#nvvmwmmam16n16k16mmarowrowf16f16-mlirnvvmwmmammaf16f16m16n16k16op>¶</a></h3><p>Warp synchronous matrix-multiply accumulate using tensor cores.</p><p>The <code>nvvm.wmma.m*n*k*.mma</code> operation performs a matrix-multiply accumulate
(mma) operation using all the threads in a warp.</p><p>The operation performed is represented as <code>D = A * B + C</code>. The operation takes
as arguments the elements of the matrices <code>A</code>, <code>B</code>, <code>C</code> and <code>D</code>, held by the
current thread. The op returns a LLVM struct which holds a part of the result
held by the current thread.</p><p>This op is meant to be used along with <code>nvvm.wmma.m16n16k16.load</code> and <code>nvvm.wmma. m16n16k16.store</code>.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%20</span> <span class=p>=</span> nvvm<span class=p>.</span>wmma<span class=p>.</span>m16n16k16<span class=p>.</span>mma<span class=p>.</span>row<span class=p>.</span>row<span class=p>.</span><span class=k>f16</span><span class=p>.</span><span class=k>f16</span> <span class=nv>%0</span><span class=p>,</span> <span class=nv>%1</span><span class=p>,</span> <span class=nv>%2</span><span class=p>,</span> <span class=nv>%3</span><span class=p>,</span> <span class=nv>%4</span><span class=p>,</span> <span class=nv>%5</span><span class=p>,</span> <span class=nv>%6</span><span class=p>,</span> <span class=nv>%7</span><span class=p>,</span> <span class=nv>%8</span><span class=p>,</span>
<span class=nv>%9</span><span class=p>,</span> <span class=nv>%10</span><span class=p>,</span> <span class=nv>%11</span><span class=p>,</span> <span class=nv>%12</span><span class=p>,</span> <span class=nv>%13</span><span class=p>,</span> <span class=nv>%14</span><span class=p>,</span> <span class=nv>%15</span><span class=p>,</span> <span class=nv>%16</span><span class=p>,</span> <span class=nv>%17</span><span class=p>,</span> <span class=nv>%18</span><span class=p>,</span> <span class=nv>%19</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;</span> <span class=p>-&gt;</span> <span class=p>!</span>llvm<span class=p>.</span>struct
<span class=p>&lt;(</span><span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;)&gt;</span>
</code></pre></div><h4 id=operands-7>Operands:&nbsp;<a class=headline-hash href=#operands-7>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>args</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h4 id=results-20>Results:&nbsp;<a class=headline-hash href=#results-20>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM structure type</td></tr></tbody></table><h3 id=nvvmwmmam16n16k16mmarowrowf32f32-mlirnvvmwmmammaf32f32m16n16k16op><code>nvvm.wmma.m16n16k16.mma.row.row.f32.f32</code> (::mlir::NVVM::WMMAMmaF32F32M16N16K16Op)&nbsp;<a class=headline-hash href=#nvvmwmmam16n16k16mmarowrowf32f32-mlirnvvmwmmammaf32f32m16n16k16op>¶</a></h3><p>Warp synchronous matrix-multiply accumulate using tensor cores.</p><p>Syntax:</p><pre><code>operation ::= `nvvm.wmma.m16n16k16.mma.row.row.f32.f32` $args attr-dict `:` functional-type($args, $res)
</code></pre><p>The <code>nvvm.wmma.m*n*k*.mma</code> operation performs a matrix-multiply accumulate
(mma) operation using all the threads in a warp.</p><p>The operation performed is represented as <code>D = A * B + C</code>. The operation takes
as arguments the elements of the matrices <code>A</code>, <code>B</code>, <code>C</code> and <code>D</code>, held by the
current thread. The op returns a LLVM struct which holds a part of the result
held by the current thread.</p><p>This op is meant to be used along with <code>nvvm.wmma.m16n16k16.load</code> and <code>nvvm.wmma. m16n16k16.store</code>.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%24</span> <span class=p>=</span> nvvm<span class=p>.</span>wmma<span class=p>.</span>m16n16k16<span class=p>.</span>mma<span class=p>.</span>row<span class=p>.</span>row<span class=p>.</span><span class=k>f32</span><span class=p>.</span><span class=k>f32</span> <span class=nv>%0</span><span class=p>,</span> <span class=nv>%1</span><span class=p>,</span> <span class=nv>%2</span><span class=p>,</span> <span class=nv>%3</span><span class=p>,</span> <span class=nv>%4</span><span class=p>,</span> <span class=nv>%5</span><span class=p>,</span> <span class=nv>%6</span><span class=p>,</span> <span class=nv>%7</span><span class=p>,</span> <span class=nv>%8</span>
<span class=nv>%9</span><span class=p>,</span> <span class=nv>%10</span><span class=p>,</span> <span class=nv>%11</span><span class=p>,</span> <span class=nv>%12</span><span class=p>,</span> <span class=nv>%13</span><span class=p>,</span> <span class=nv>%14</span><span class=p>,</span> <span class=nv>%15</span><span class=p>,</span> <span class=nv>%16</span><span class=p>,</span> <span class=nv>%17</span><span class=p>,</span> <span class=nv>%18</span><span class=p>,</span> <span class=nv>%19</span><span class=p>,</span> <span class=nv>%20</span><span class=p>,</span> <span class=nv>%21</span><span class=p>,</span> <span class=nv>%22</span><span class=p>,</span> <span class=nv>%23</span> <span class=p>:</span>
<span class=p>(</span><span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;,</span>
<span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;,</span>
<span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;,</span>
<span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=k>f32</span><span class=p>,</span> <span class=k>f32</span><span class=p>,</span> <span class=k>f32</span><span class=p>,</span> <span class=k>f32</span><span class=p>,</span> <span class=k>f32</span><span class=p>,</span> <span class=k>f32</span><span class=p>,</span> <span class=k>f32</span><span class=p>,</span> <span class=k>f32</span><span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>llvm<span class=p>.</span>struct<span class=p>&lt;(</span><span class=k>f32</span><span class=p>,</span>
<span class=k>f32</span><span class=p>,</span> <span class=k>f32</span><span class=p>,</span> <span class=k>f32</span><span class=p>,</span> <span class=k>f32</span><span class=p>,</span> <span class=k>f32</span><span class=p>,</span> <span class=k>f32</span><span class=p>,</span> <span class=k>f32</span><span class=p>)&gt;</span>
</code></pre></div><h4 id=operands-8>Operands:&nbsp;<a class=headline-hash href=#operands-8>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>args</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h4 id=results-21>Results:&nbsp;<a class=headline-hash href=#results-21>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM structure type</td></tr></tbody></table><h3 id=nvvmwmmam16n16k16storedf16rowstride-mlirnvvmwmmastoref16m16n16k16op><code>nvvm.wmma.m16n16k16.store.d.f16.row.stride</code> (::mlir::NVVM::WMMAStoreF16M16N16K16Op)&nbsp;<a class=headline-hash href=#nvvmwmmam16n16k16storedf16rowstride-mlirnvvmwmmastoref16m16n16k16op>¶</a></h3><p>Warp synchronous matrix store</p><p>Syntax:</p><pre><code>operation ::= `nvvm.wmma.m16n16k16.store.d.f16.row.stride` $args attr-dict `:` type($args)
</code></pre><p>The <code>nvvm.wmma.m*n*k*.store</code> operation stores a matrix collectively using
all the threads in a warp.</p><p>The operation takes as arguments the address to where the matrix elements are
to be stored, a stride and the elements to store, held by the current thread.
The stride argument represents the leading dimension of the destination matrix.
The address and the stride are required to be the same across all threads in the
warp.</p><p>This op is meant to be used along with <code>nvvm.wmma.m16n16k16.load</code> and
<code>nvvm.wmma.m16n16k16.mma</code>.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir>nvvm<span class=p>.</span>wmma<span class=p>.</span>m16n16k16<span class=p>.</span>stored<span class=p>.</span><span class=k>f16</span><span class=p>.</span>row<span class=p>.</span>stride <span class=nv>%0</span><span class=p>,</span> <span class=nv>%1</span><span class=p>,</span> <span class=nv>%2</span><span class=p>,</span> <span class=nv>%3</span><span class=p>,</span> <span class=nv>%4</span><span class=p>,</span> <span class=nv>%5</span><span class=p>,</span> <span class=nv>%6</span> <span class=p>:</span> <span class=p>!</span>llvm<span class=p>.</span>ptr<span class=p>&lt;</span><span class=k>i32</span><span class=p>,</span> <span class=m>3</span><span class=p>&gt;,</span>
<span class=p>!</span>llvm<span class=p>.</span>struct<span class=p>&lt;(</span>vec<span class=p>&lt;</span><span class=m>2 x</span> half<span class=p>&gt;,</span> vec<span class=p>&lt;</span><span class=m>2 x</span> half<span class=p>&gt;,</span> vec<span class=p>&lt;</span><span class=m>2 x</span> half<span class=p>&gt;,</span> vec<span class=p>&lt;</span><span class=m>2 x</span> half<span class=p>&gt;)&gt;,</span> <span class=p>!</span>llvm<span class=p>.</span><span class=k>i32</span>
</code></pre></div><h4 id=operands-9>Operands:&nbsp;<a class=headline-hash href=#operands-9>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>args</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h3 id=nvvmwmmam16n16k16storedf32rowstride-mlirnvvmwmmastoref32m16n16k16op><code>nvvm.wmma.m16n16k16.store.d.f32.row.stride</code> (::mlir::NVVM::WMMAStoreF32M16N16K16Op)&nbsp;<a class=headline-hash href=#nvvmwmmam16n16k16storedf32rowstride-mlirnvvmwmmastoref32m16n16k16op>¶</a></h3><p>Warp synchronous matrix store</p><p>Syntax:</p><pre><code>operation ::= `nvvm.wmma.m16n16k16.store.d.f32.row.stride` $args attr-dict `:` type($args)
</code></pre><p>The <code>nvvm.wmma.m*n*k*.store</code> operation stores a matrix collectively using
all the threads in a warp.</p><p>The operation takes as arguments the address to where the matrix elements are
to be stored, a stride and the elements to store, held by the current thread.
The stride argument represents the leading dimension of the destination matrix.
The address and the stride are required to be the same across all threads in the
warp.</p><p>This op is meant to be used along with <code>nvvm.wmma.m16n16k16.load</code> and
<code>nvvm.wmma.m16n16k16.mma</code>.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir>nvvm<span class=p>.</span>wmma<span class=p>.</span>m16n16k16<span class=p>.</span>store<span class=p>.</span>d<span class=p>.</span><span class=k>f32</span><span class=p>.</span>row<span class=p>.</span>stride <span class=nv>%0</span><span class=p>,</span> <span class=nv>%1</span><span class=p>,</span> <span class=nv>%2</span><span class=p>,</span> <span class=nv>%3</span><span class=p>,</span> <span class=nv>%4</span><span class=p>,</span> <span class=nv>%5</span><span class=p>,</span> <span class=nv>%6</span><span class=p>,</span> <span class=nv>%7</span><span class=p>,</span> <span class=nv>%8</span><span class=p>,</span> <span class=nv>%9</span><span class=p>,</span>
<span class=nv>%10</span> <span class=p>:</span> <span class=p>!</span>llvm<span class=p>.</span>ptr<span class=p>&lt;</span><span class=k>i32</span><span class=p>,</span> <span class=m>3</span><span class=p>&gt;,</span> <span class=p>!</span>llvm<span class=p>.</span>struct<span class=p>&lt;(</span><span class=k>f32</span><span class=p>,</span> <span class=k>f32</span><span class=p>,</span> <span class=k>f32</span><span class=p>,</span> <span class=k>f32</span><span class=p>,</span> <span class=k>f32</span><span class=p>,</span> <span class=k>f32</span><span class=p>,</span> <span class=k>f32</span><span class=p>,</span> <span class=k>f32</span><span class=p>)&gt;,</span>
<span class=p>!</span>llvm<span class=p>.</span><span class=k>i32</span>
</code></pre></div><h4 id=operands-10>Operands:&nbsp;<a class=headline-hash href=#operands-10>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>args</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><h3 id=nvvmreadptxsregwarpsize-mlirnvvmwarpsizeop><code>nvvm.read.ptx.sreg.warpsize</code> (::mlir::NVVM::WarpSizeOp)&nbsp;<a class=headline-hash href=#nvvmreadptxsregwarpsize-mlirnvvmwarpsizeop>¶</a></h3><p>Syntax:</p><pre><code>operation ::= `nvvm.read.ptx.sreg.warpsize` attr-dict `:` type($res)
</code></pre><h4 id=results-22>Results:&nbsp;<a class=headline-hash href=#results-22>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>LLVM dialect-compatible type</td></tr></tbody></table><div class=edit-meta><br></div><nav class=pagination><a class="nav nav-prev" href=/docs/Dialects/MemRef/ title="'memref' Dialect"><i class="fas fa-arrow-left" aria-hidden=true></i>Prev - 'memref' Dialect</a>
<a class="nav nav-next" href=/docs/Dialects/OpenMPDialect/ title="'omp' Dialect">Next - 'omp' Dialect <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://mlir.llvm.org/>Home</a></li><li><a href=/talks/>Talks and Related Publications</a></li><li><a href=/users/>Users of MLIR</a></li><li class=has-sub-menu><a href=/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/getting_started/Debugging/>Debugging</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li><a href=/getting_started/Contributing/>How to Contribute</a></li><li><a href=/getting_started/DeveloperGuide/>Developer Guide</a></li><li><a href=/getting_started/openprojects/>Open Projects</a></li><li><a href=/getting_started/Glossary/>Glossary</a></li><li><a href=/getting_started/TestingGuide/>Testing Guide</a></li></ul></li><li class="parent has-sub-menu"><a href=/docs/>Code Documentation<span class="mark opened">-</span></a><ul class=sub-menu><li class=has-sub-menu><a href=/docs/Bindings/>Bindings<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Bindings/Python/>MLIR Python Bindings</a></li></ul></li><li class=has-sub-menu><a href=/docs/Tools/>Tools<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Tools/MLIRLSP/>MLIR : Language Server Protocol</a></li><li><a href=/docs/Tools/mlir-reduce/>MLIR Reduce</a></li></ul></li><li><a href=/docs/BufferDeallocationInternals/>Buffer Deallocation - Internals</a></li><li><a href=/docs/Bufferization/>Bufferization</a></li><li><a href=/docs/LLVMDialectMemRefConvention/>Built-in Function and MemRef Calling Convention</a></li><li><a href=/docs/ConversionToLLVMDialect/>Conversion to the LLVM Dialect</a></li><li><a href=/docs/DataLayout/>Data Layout Modeling</a></li><li><a href=/docs/DebugActions/>Debug Actions</a></li><li><a href=/docs/Diagnostics/>Diagnostic Infrastructure</a></li><li><a href=/docs/DialectConversion/>Dialect Conversion</a></li><li class="parent has-sub-menu"><a href=/docs/Dialects/>Dialects<span class="mark opened">-</span></a><ul class=sub-menu><li><a href=/docs/Dialects/DLTIDialect/></a></li><li><a href=/docs/Dialects/OpenACCDialect/>'acc' Dialect</a></li><li><a href=/docs/Dialects/Affine/>'affine' Dialect</a></li><li><a href=/docs/Dialects/AMX/>'amx' Dialect</a></li><li><a href=/docs/Dialects/ArmNeon/>'arm_neon' Dialect</a></li><li><a href=/docs/Dialects/ArmSVE/>'arm_sve' Dialect</a></li><li><a href=/docs/Dialects/AsyncDialect/>'async' Dialect</a></li><li><a href=/docs/Dialects/ComplexOps/>'complex' Dialect</a></li><li><a href=/docs/Dialects/EmitC/>'emitc' Dialect</a></li><li><a href=/docs/Dialects/GPU/>'gpu' Dialect</a></li><li class=has-sub-menu><a href=/docs/Dialects/Linalg/>'linalg' Dialect<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Dialects/Linalg/OpDSL/>Linalg OpDSL</a></li></ul></li><li><a href=/docs/Dialects/LLVM/>'llvm' Dialect</a></li><li><a href=/docs/Dialects/MathOps/>'math' Dialect</a></li><li><a href=/docs/Dialects/MemRef/>'memref' Dialect</a></li><li class=active><a href=/docs/Dialects/NVVMDialect/>'nvvm' Dialect</a></li><li><a href=/docs/Dialects/OpenMPDialect/>'omp' Dialect</a></li><li><a href=/docs/Dialects/PDLOps/>'pdl' Dialect</a></li><li><a href=/docs/Dialects/PDLInterpOps/>'pdl_interp' Dialect</a></li><li><a href=/docs/Dialects/QuantDialect/>'quant' Dialect</a></li><li><a href=/docs/Dialects/ROCDLDialect/>'rocdl' Dialect</a></li><li><a href=/docs/Dialects/SCFDialect/>'scf' Dialect</a></li><li><a href=/docs/Dialects/ShapeDialect/>'shape' Dialect</a></li><li><a href=/docs/Dialects/SparseTensorOps/>'sparse_tensor' Dialect</a></li><li><a href=/docs/Dialects/SPIR-V/>'spv' Dialect</a></li><li><a href=/docs/Dialects/Standard/>'std' Dialect</a></li><li><a href=/docs/Dialects/TensorOps/>'tensor' Dialect</a></li><li><a href=/docs/Dialects/Vector/>'vector' Dialect</a></li><li><a href=/docs/Dialects/X86Vector/>'x86vector' Dialect</a></li><li><a href=/docs/Dialects/Builtin/>Builtin Dialect</a></li><li><a href=/docs/Dialects/TOSA/>Tensor Operator Set Architecture (TOSA) Dialect</a></li></ul></li><li><a href=/docs/Interfaces/>Interfaces</a></li><li><a href=/docs/CAPI/>MLIR C API</a></li><li><a href=/docs/LangRef/>MLIR Language Reference</a></li><li><a href=/docs/Canonicalization/>Operation Canonicalization</a></li><li><a href=/docs/OpDefinitions/>Operation Definition Specification (ODS)</a></li><li><a href=/docs/PassManagement/>Pass Infrastructure</a></li><li><a href=/docs/Passes/>Passes</a></li><li><a href=/docs/PatternRewriter/>Pattern Rewriting : Generic DAG-to-DAG Rewriting</a></li><li><a href=/docs/Quantization/>Quantization</a></li><li class=has-sub-menu><a href=/docs/Rationale/>Rationale<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Rationale/RationaleGenericDAGRewriter/>Generic DAG Rewriter Infrastructure Rationale</a></li><li><a href=/docs/Rationale/RationaleLinalgDialect/>Linalg Dialect Rationale: The Case For Compiler-Friendly Custom Operations</a></li><li><a href=/docs/Rationale/Rationale/>MLIR Rationale</a></li><li><a href=/docs/Rationale/MLIRForGraphAlgorithms/>MLIR: Incremental Application to Graph Algorithms in ML Frameworks</a></li><li><a href=/docs/Rationale/RationaleSimplifiedPolyhedralForm/>MLIR: The case for a simplified polyhedral form</a></li><li><a href=/docs/Rationale/UsageOfConst/>Usage of 'const' in MLIR, for core IR types</a></li></ul></li><li><a href=/docs/ShapeInference/>Shape Inference</a></li><li><a href=/docs/SPIRVToLLVMDialectConversion/>SPIR-V Dialect to LLVM Dialect conversion manual</a></li><li><a href=/docs/SymbolsAndSymbolTables/>Symbols and Symbol Tables</a></li><li><a href=/docs/DeclarativeRewrites/>Table-driven Declarative Rewrite Rule (DRR)</a></li><li><a href=/docs/Traits/>Traits</a></li><li class=has-sub-menu><a href=/docs/Tutorials/>Tutorials<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Tutorials/CreatingADialect/>Creating a Dialect</a></li><li><a href=/docs/Tutorials/DefiningAttributesAndTypes/>Defining Dialect Attributes and Types</a></li><li><a href=/docs/Tutorials/QuickstartRewrites/>Quickstart tutorial to adding MLIR graph rewrite</a></li><li class=has-sub-menu><a href=/docs/Tutorials/Toy/>Toy Tutorial<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Tutorials/Toy/Ch-1/>Chapter 1: Toy Language and AST</a></li><li><a href=/docs/Tutorials/Toy/Ch-2/>Chapter 2: Emitting Basic MLIR</a></li><li><a href=/docs/Tutorials/Toy/Ch-3/>Chapter 3: High-level Language-Specific Analysis and Transformation</a></li><li><a href=/docs/Tutorials/Toy/Ch-4/>Chapter 4: Enabling Generic Transformation with Interfaces</a></li><li><a href=/docs/Tutorials/Toy/Ch-5/>Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization</a></li><li><a href=/docs/Tutorials/Toy/Ch-6/>Chapter 6: Lowering to LLVM and CodeGeneration</a></li><li><a href=/docs/Tutorials/Toy/Ch-7/>Chapter 7: Adding a Composite Type to Toy</a></li></ul></li><li><a href=/docs/Tutorials/UnderstandingTheIRStructure/>Understanding the IR Structure</a></li><li><a href=/docs/Tutorials/DataFlowAnalysis/>Writing DataFlow Analyses in MLIR</a></li></ul></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i><i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>